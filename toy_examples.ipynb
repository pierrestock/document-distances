{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "__Fast computation of document distances using optimal transportation__\n",
    "========================================================\n",
    "\n",
    "We illustrate the algorithm described in the report on a toy examples to understand the influence of the different parameters. For further details we refer to the pdf report as well as the python files.\n",
    "$\\newcommand{\\dotp}[2]{\\langle #1, #2 \\rangle}$\n",
    "$\\newcommand{\\enscond}[2]{\\lbrace #1, #2 \\rbrace}$\n",
    "$\\newcommand{\\pd}[2]{ \\frac{ \\partial #1}{\\partial #2} }$\n",
    "$\\newcommand{\\umin}[1]{\\underset{#1}{\\min}\\;}$\n",
    "$\\newcommand{\\umax}[1]{\\underset{#1}{\\max}\\;}$\n",
    "$\\newcommand{\\umin}[1]{\\underset{#1}{\\min}\\;}$\n",
    "$\\newcommand{\\uargmin}[1]{\\underset{#1}{argmin}\\;}$\n",
    "$\\newcommand{\\norm}[1]{\\|#1\\|}$\n",
    "$\\newcommand{\\abs}[1]{\\left|#1\\right|}$\n",
    "$\\newcommand{\\choice}[1]{ \\left\\{  \\begin{array}{l} #1 \\end{array} \\right. }$\n",
    "$\\newcommand{\\pa}[1]{\\left(#1\\right)}$\n",
    "$\\newcommand{\\diag}[1]{{diag}\\left( #1 \\right)}$\n",
    "$\\newcommand{\\qandq}{\\quad\\text{and}\\quad}$\n",
    "$\\newcommand{\\qwhereq}{\\quad\\text{where}\\quad}$\n",
    "$\\newcommand{\\qifq}{ \\quad \\text{if} \\quad }$\n",
    "$\\newcommand{\\qarrq}{ \\quad \\Longrightarrow \\quad }$\n",
    "$\\newcommand{\\ZZ}{\\mathbb{Z}}$\n",
    "$\\newcommand{\\CC}{\\mathbb{C}}$\n",
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\EE}{\\mathbb{E}}$\n",
    "$\\newcommand{\\Zz}{\\mathcal{Z}}$\n",
    "$\\newcommand{\\Ww}{\\mathcal{W}}$\n",
    "$\\newcommand{\\Vv}{\\mathcal{V}}$\n",
    "$\\newcommand{\\Nn}{\\mathcal{N}}$\n",
    "$\\newcommand{\\NN}{\\mathcal{N}}$\n",
    "$\\newcommand{\\Hh}{\\mathcal{H}}$\n",
    "$\\newcommand{\\Bb}{\\mathcal{B}}$\n",
    "$\\newcommand{\\Ee}{\\mathcal{E}}$\n",
    "$\\newcommand{\\Cc}{\\mathcal{C}}$\n",
    "$\\newcommand{\\Gg}{\\mathcal{G}}$\n",
    "$\\newcommand{\\Ss}{\\mathcal{S}}$\n",
    "$\\newcommand{\\Pp}{\\mathcal{P}}$\n",
    "$\\newcommand{\\Ff}{\\mathcal{F}}$\n",
    "$\\newcommand{\\Xx}{\\mathcal{X}}$\n",
    "$\\newcommand{\\Mm}{\\mathcal{M}}$\n",
    "$\\newcommand{\\Ii}{\\mathcal{I}}$\n",
    "$\\newcommand{\\Dd}{\\mathcal{D}}$\n",
    "$\\newcommand{\\Ll}{\\mathcal{L}}$\n",
    "$\\newcommand{\\Tt}{\\mathcal{T}}$\n",
    "$\\newcommand{\\si}{\\sigma}$\n",
    "$\\newcommand{\\al}{\\alpha}$\n",
    "$\\newcommand{\\la}{\\lambda}$\n",
    "$\\newcommand{\\ga}{\\gamma}$\n",
    "$\\newcommand{\\Ga}{\\Gamma}$\n",
    "$\\newcommand{\\La}{\\Lambda}$\n",
    "$\\newcommand{\\si}{\\sigma}$\n",
    "$\\newcommand{\\Si}{\\Sigma}$\n",
    "$\\newcommand{\\be}{\\beta}$\n",
    "$\\newcommand{\\de}{\\delta}$\n",
    "$\\newcommand{\\De}{\\Delta}$\n",
    "$\\newcommand{\\phi}{\\varphi}$\n",
    "$\\newcommand{\\th}{\\theta}$\n",
    "$\\newcommand{\\om}{\\omega}$\n",
    "$\\newcommand{\\Om}{\\Omega}$\n",
    "$\\newcommand{\\eqdef}{\\equiv}$\n",
    "$\\newcommand\\ones{\\mathbf{1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim as gs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from compute_distance import distance\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "data_path = \"/home/ubuntu/pcs/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief description of Sinkhorn's algorithm\n",
    "---------------------------------------------\n",
    "\n",
    "We use the approach explained in [Kusner](#biblio) consisting in solving the transportation problem between histograms of texts. After a preprocessing step, each document is mapped to an histogram counting the occurence of the words in that document (using as support all the words appearing in at least on of the documents). \n",
    "\n",
    "The words are then embedded in a high-dimentional space using the word2vec model. The distance between the two docments is then the minimum transportation cost between the two histograms :\n",
    "\n",
    "$$d = \\min_{\\pi \\in \\Pi(p,q)}\\sum_{i,j}\\pi_{i,j} C_{i,j}$$\n",
    "\n",
    "with the weight constraints \n",
    "\n",
    "$$\\pi \\in \\Pi(p,q) = \\enscond{\\pi \\in (\\RR^+)^{N \\times N}}{ \\pi \\ones = p, \\pi^T \\ones = q }$$\n",
    "\n",
    "using\n",
    "- The euclidian norm in the embedding space as transportation cost $C_{i,j}$,\n",
    "- Weights $p$ and $q$ representing the occurences of words on each document.\n",
    "\n",
    "Instead of directly solving this problem, we rather implement the Sinkhorn's algorithm described in [Sinkhorn](#biblio) to compute the optimal policy $\\pi^{\\star}$. It consists in solving an entropic-regularized version of the transportation problem:\n",
    "\n",
    "$$\\pi^{\\star} = \\text{argmin}_{\\pi \\in \\Pi(p,q)}\\langle C, \\pi \\rangle - \\lambda E(\\pi)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word2vec Model\n",
    "-------------------------\n",
    "\n",
    "We will leverage the recent works of Mikolov et al. [10] by embedding the words in a 300-dimensional space. The subsequent freely available model generates low-dimensional word embeddings using a 2-layers shallow neural network that has been trained on approximately 100 billion words. The authors demonstrated that the model learned high-quality word representations that are able to capture precise syntactic and semantic meanings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model *(may take some time)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gs.models.Word2Vec.load_word2vec_format(data_path + 'word2vec/GoogleNews-vectors-negative300.bin.gz', binary=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word is represented by a vector in a 300-dimensional embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Dimension of embedding space: %d\" %len(model[\"man\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "words = [\"China\", \"Beijing\", \"Russia\", \"Moscow\", \"Japan\", \"Tokyo\", \"Turkey\", \"Ankara\", \"Poland\", \"Warsaw\", \"Germany\", \"Berlin\", \"France\", \"Paris\", \"Italy\", \"Rome\", \"Greece\", \"Athens\"]\n",
    "X = model[words]\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X[:, 0], X[:, 1], s = 30)\n",
    "\n",
    "# annotations\n",
    "for i in range(len(words)):\n",
    "    eps = .1\n",
    "    plt.text(X[i, 0] + eps, X[i, 1] - eps, words[i], fontsize = 12)\n",
    "\n",
    "# vectors\n",
    "for i in range(len(words)):\n",
    "    if i % 2 == 0:\n",
    "        plt.plot(X[i:(i + 2), 0], X[i:(i + 2), 1], linewidth = 1.5)\n",
    "    \n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find word synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['great'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform algebraic calculus in the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['Messi', 'tennis'], negative=['football'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other fun functionnalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.doesnt_match(\"Debussy Satie Ravel Bach\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustration of the algorithm on news articles\n",
    "------------------------------\n",
    "Load some articles related to sports or politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(data_path + \"sport1.txt\", encoding='utf-8') as f:\n",
    "    sport1 = f.read()\n",
    "with open(data_path + \"sport2.txt\", encoding='utf-8') as f:\n",
    "    sport2 = f.read()\n",
    "with open(data_path + \"politics1.txt\", encoding='utf-8') as f:\n",
    "    politics1 = f.read()\n",
    "with open(data_path + \"politics2.txt\", encoding='utf-8') as f:\n",
    "    politics2 = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute pairwise distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Distance(sport1, sport2) = %.2f \" %distance(sport1, sport2, 1, 100, data_path))\n",
    "print(\"Distance(politics1, politics2) = %.2f \" %distance(politics1, politics2, 1, 100, data_path))\n",
    "print(\"Distance(sport1, politics2) = %.2f \" %distance(sport1, politics2, 1, 100, data_path))\n",
    "print(\"Distance(politics1, sport2) = %.2f \" %distance(politics1, sport2, 1, 100, data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Influence of the regularization parameter\n",
    "---------------------------------------------\n",
    "Influence of $\\lambda$ this parameter has to be tuned carefully depending on the type of documents we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambda_list = np.logspace(-2,2,10)\n",
    "niter_list = [10, 50, 100, 300]\n",
    "plt.figure(figsize = (16,12))\n",
    "    \n",
    "for it in range(len(niter_list)):\n",
    "    niter = niter_list[it]\n",
    "    results = np.zeros([len(lambda_list), 4])\n",
    "\n",
    "    for i in range(len(lambda_list)):\n",
    "        lambd = lambda_list[i]\n",
    "        results[i, 0] =  distance(sport1, sport2, lambd, niter)\n",
    "        results[i, 1] =  distance(politics1, politics2, lambd, niter)\n",
    "        results[i, 2] =  distance(sport1, politics2, lambd, niter)\n",
    "        results[i, 3] =  distance(politics1, sport2, lambd, niter)\n",
    "    \n",
    "    plt.subplot(2,2,it + 1)\n",
    "    plt.plot(results[:, 0] , color = 'red')\n",
    "    plt.plot(results[:, 1] , color = 'blue')\n",
    "    plt.plot(results[:, 2] , 'r--', color = 'black')\n",
    "    plt.plot(results[:, 3] , 'r-.', color = 'black')\n",
    "    plt.xticks(np.arange(10), np.around(lambda_list, 2))\n",
    "    plt.xlabel(\"$\\lambda$\")\n",
    "    plt.ylabel(\"Distance\")\n",
    "    plt.title(\"Number of iterations: %d\" %niter)\n",
    "    plt.legend(['sport1 - sport2', 'politics1 - politics2', 'sport1 - politics2', 'politics1 - sport2'], loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Influence of the embedding metric\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_list = np.logspace(0,1,10)\n",
    "niter_list = [10, 50, 100, 300]\n",
    "plt.figure(figsize = (16,12))\n",
    "    \n",
    "for it in range(len(niter_list)):\n",
    "    niter = niter_list[it]\n",
    "    results = np.zeros([len(p_list), 4])\n",
    "\n",
    "    for i in range(len(lambda_list)):\n",
    "        lambd = lambda_list[i]\n",
    "        results[i, 0] =  distance(sport1, sport2, lambd, niter)\n",
    "        results[i, 1] =  distance(politics1, politics2, lambd, niter)\n",
    "        results[i, 2] =  distance(sport1, politics2, lambd, niter)\n",
    "        results[i, 3] =  distance(politics1, sport2, lambd, niter)\n",
    "    \n",
    "    plt.subplot(2,2,it + 1)\n",
    "    plt.plot(results[:, 0] , color = 'red')\n",
    "    plt.plot(results[:, 1] , color = 'blue')\n",
    "    plt.plot(results[:, 2] , 'r--', color = 'black')\n",
    "    plt.plot(results[:, 3] , 'r-.', color = 'black')\n",
    "    plt.xticks(np.arange(10), np.around(lambda_list, 2))\n",
    "    plt.xlabel(\"$\\lambda$\")\n",
    "    plt.ylabel(\"Distance\")\n",
    "    plt.title(\"Number of iterations: %d\" %niter)\n",
    "    plt.legend(['sport1 - sport2', 'politics1 - politics2', 'sport1 - politics2', 'politics1 - sport2'], loc=4)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
